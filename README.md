# llm-stack-local

# üß† Implementaci√≥n de agentes de IA con LLMs locales open source

Este repositorio contiene la implementaci√≥n de un entorno de **IA local** para el caso de uso  

**interacci√≥n con modelos de lenguaje y recuperaci√≥n aumentada (RAG)**.  

que hace parte del proyecto de grado **"Implementaci√≥n de agentes de IA con LLMs locales en entornos seguros"** de la **Pontificia Universidad Javeriana**.

La arquitectura se basa en tres componentes principales:

- **[Ollama](https://ollama.ai/)** ‚Üí Motor para ejecutar modelos de lenguaje grandes (**LLMs**) localmente.
- **[Open WebUI](https://github.com/open-webui/open-webui)** ‚Üí Interfaz web para interacci√≥n con LLMs y gesti√≥n de memoria.
- **[Qdrant](https://qdrant.tech/)** ‚Üí Base de datos vectorial para almacenamiento y b√∫squeda sem√°ntica de embeddings.

El modelo configurado por defecto es **Mistral**, un LLM ligero y eficiente, ideal para entornos con recursos limitados.

## ‚ñ∂Ô∏è Instrucciones de uso

1. Clonar este repositorio:
   ```bash
   git clone https://github.com/<tu-usuario>/<tu-repo>.git
   cd <tu-repo>
2. Levantar los servicios:
   ```bash
    docker compose up -d
3. Acceder a la interfaz web:
    http://localhost:3000

## üîß Servicios
Ollama

- API: http://localhost:11434

- Persistencia en volumen: ollama_data

- Modelo inicial: mistral (se descarga autom√°ticamente la primera vez).

Comandos √∫tiles:

# Ingresar al contenedor
docker exec -it ollama bash

# Listar modelos instalados
ollama list

# Descargar otro modelo
ollama pull llama2

Open WebUI

- Interfaz: http://localhost:3000

- Conexiones configuradas:

    - OLLAMA_BASE_URL=http://ollama:11434

    - MEMORY_BACKEND=qdrant

- Modelo por defecto: mistral

- Configuraci√≥n avanzada (variables de entorno adicionales):

    - DEFAULT_MODEL ‚Üí Modelo seleccionado al inicio.

    - MEMORY_BACKEND ‚Üí Motor de memoria (qdrant, sqlite, etc.).

    - QDRANT_URL ‚Üí Endpoint de Qdrant.

Qdrant

- API HTTP: http://localhost:6333

- API gRPC: localhost:6334

- Persistencia en volumen: qdrant_data

## üß™ Caso de uso: RAG (Retrieval Augmented Generation)

1. Carga de documentos en Qdrant mediante embeddings generados por Ollama.

2. Consulta sem√°ntica desde Open WebUI:

    - El prompt del usuario se convierte en un embedding.

    - Se busca en Qdrant el contexto m√°s relevante.

    - El contexto se concatena al prompt antes de pasarlo al modelo Mistral.

3. Respuesta final generada con conocimiento aumentado.

## üìö  Referencias

Ollama Docs

Open WebUI

Qdrant Docs

---

## ‚öôÔ∏è Arquitectura de la soluci√≥n

```mermaid
flowchart LR
    subgraph User["üë§ Usuario"]
      A[Open WebUI] -->|prompt| B[Ollama]
      A -->|consulta sem√°ntica| C[Qdrant]
    end

    subgraph Backend["üñ•Ô∏è Backend Local"]
      B[Ollama] -->|respuesta| A
      C[Qdrant] -->|documentos relevantes| A
    end
