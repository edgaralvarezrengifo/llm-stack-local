# llm-stack-local

# ðŸ§  ImplementaciÃ³n de agentes de IA con LLMs locales open source

Este repositorio contiene la implementaciÃ³n de un entorno de **IA local** para el caso de uso  

**interacciÃ³n con modelos de lenguaje y recuperaciÃ³n aumentada (RAG)**.  

que hace parte del proyecto de grado **"ImplementaciÃ³n de agentes de IA con LLMs locales en entornos seguros"** de la **Pontificia Universidad Javeriana**.

La arquitectura se basa en tres componentes principales:

- **[Ollama](https://ollama.ai/)** â†’ Motor para ejecutar modelos de lenguaje grandes (**LLMs**) localmente.
- **[Open WebUI](https://github.com/open-webui/open-webui)** â†’ Interfaz web para interacciÃ³n con LLMs y gestiÃ³n de memoria.
- **[Qdrant](https://qdrant.tech/)** â†’ Base de datos vectorial para almacenamiento y bÃºsqueda semÃ¡ntica de embeddings.

El modelo configurado por defecto es **Mistral**, un LLM ligero y eficiente, ideal para entornos con recursos limitados.

---

## âš™ï¸ Arquitectura de la soluciÃ³n

```mermaid
flowchart LR
    subgraph User["ðŸ‘¤ Usuario"]
      A[Open WebUI] -->|prompt| B[Ollama]
      A -->|consulta semÃ¡ntica| C[Qdrant]
    end

    subgraph Backend["ðŸ–¥ï¸ Backend Local"]
      B[Ollama] -->|respuesta| A
      C[Qdrant] -->|documentos relevantes| A
    end
